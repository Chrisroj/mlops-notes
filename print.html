<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js rust">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>test-book</title>
        <meta name="robots" content="noindex" />
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "rust" : "rust";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="Module1/1_intro.html"><strong aria-hidden="true">1.</strong> Introduction to MLOps</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="Module1/2_aws.html"><strong aria-hidden="true">1.1.</strong> VM Instance in AWS</a></li><li class="chapter-item expanded "><a href="Module1/3_dependencies.html"><strong aria-hidden="true">1.2.</strong> Install Dependencies</a></li><li class="chapter-item expanded "><a href="Module1/4_overview.html"><strong aria-hidden="true">1.3.</strong> Course Overview</a></li><li class="chapter-item expanded "><a href="Module1/5_maturity.html"><strong aria-hidden="true">1.4.</strong> MLOps Maturity Model</a></li></ol></li><li class="chapter-item expanded "><a href="Module2/1_intro_exptracking.html"><strong aria-hidden="true">2.</strong> Introduction to Experiment tracking</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="Module2/2_start_mlflow.html"><strong aria-hidden="true">2.1.</strong> Getting Started with MLflow</a></li><li class="chapter-item expanded "><a href="Module2/3_track_single.html"><strong aria-hidden="true">2.2.</strong> Tracking a Single Experiment</a></li><li class="chapter-item expanded "><a href="Module2/4_track_hyper.html"><strong aria-hidden="true">2.3.</strong> Tracking a Multiples Experiments</a></li><li class="chapter-item expanded "><a href="Module2/5_autologging.html"><strong aria-hidden="true">2.4.</strong> Autologging</a></li><li class="chapter-item expanded "><a href="Module2/6_log_models.html"><strong aria-hidden="true">2.5.</strong> Log Models</a></li><li class="chapter-item expanded "><a href="Module2/7_model_registry.html"><strong aria-hidden="true">2.6.</strong> Model Registry</a></li><li class="chapter-item expanded "><a href="Module2/8_mlflow_benefits.html"><strong aria-hidden="true">2.7.</strong> MLflow: Benefits, Limitations and Alternatives</a></li></ol></li><li class="chapter-item expanded "><a href="Appendixes/A_gcp.html"><strong aria-hidden="true">3.</strong> Appendix A: VM Instance in GCP</a></li><li class="chapter-item expanded "><a href="Appendixes/B_conda.html"><strong aria-hidden="true">4.</strong> Appendix B: Conda Environments</a></li><li class="chapter-item expanded "><a href="Appendixes/C_mlflow_aws.html"><strong aria-hidden="true">5.</strong> Appendix C: MLflow on AWS</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">test-book</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction-to-mlops"><a class="header" href="#introduction-to-mlops">Introduction to MLOps</a></h1>
<p>All the next notes are about <a href="https://github.com/DataTalksClub/mlops-zoomcamp">MLOps Zoomcamp</a> course.</p>
<p><a href="https://youtu.be/s0uaFZSzwfI?list=PL3MmuxUbc_hIUISrluw_A7wDSmfOhErJK">Video source</a></p>
<p><em><strong>MLOps</strong></em> is a <em>set of best practices</em> for bringing Machine Learning to production.</p>
<p>Machine Learning projects can be simpplified to just 3 steps:</p>
<ol>
<li><em><strong>Design</strong></em> - is ML the right tool for solving our problem?
<ul>
<li><em>We want to predict the duration of a taxi trip. Do we need to use ML or can we used a simpler rule-based model?</em></li>
</ul>
</li>
<li><em><strong>Train</strong></em> - if we do need ML, then we train and evaluate the best model.</li>
<li><em><strong>Operate</strong></em> - model deployment, management and monitoring.</li>
</ol>
<p>MLOps is helpful in all 3 stages.</p>
<p><img src="Module1/../Images/Module1/mlops_process.jpg" alt="mlops" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="vm-instance-in-aws"><a class="header" href="#vm-instance-in-aws">VM Instance in AWS</a></h1>
<p>Before to start working with MLOps you should use a VM instance to work with and setup the environment to work.</p>
<p>You can prepare your environment in your local machine, but in our case we are going to set up a VM instance in AWS. You need to have an AWS account(if you want to do the same in GCP go to <a href="Module1/../Appendixes/A_gcp.html">VM Instance in GCP</a>)</p>
<ol>
<li>
<p><strong>Create EC2 instance:</strong></p>
<p>Go to EC2 service and click in launch instance(orange button), now config the VM as:</p>
<ul>
<li>Name: <code>mlops-zoomcamp</code></li>
<li>Amazon Machine Image: <code>Ubuntu Server 22.04 LTS (HVM), SSD Volume Type</code></li>
<li>Architecture: <code>64-bit (x86)</code></li>
<li>Instance type: <code>t2.large</code></li>
<li>Create and select a key pair:
<ul>
<li>Key pair name: <code>asus-laptop</code></li>
<li>Key pair type: <code>RSA</code></li>
<li>Private key file format: <code>.pem</code></li>
</ul>
</li>
<li>Configure Storage: <code>1x 30 Gib gp2 Root Volume</code>.</li>
</ul>
</li>
<li>
<p><strong>Copy and paste <code>.pem</code> file:</strong> </p>
<p>When you create a key pair a <code>.pem</code> will downloaded automatically, you will have to copy and paste this file to your  <code>~/.ssh</code> directory in your local machine.</p>
</li>
<li>
<p><strong>Connect to VM Instance:</strong> </p>
<p>Go to <code>~./.ssh</code> directory and locate the <code>config</code> file type nano <code>~/.ssh/config</code> copy and paste:</p>
<pre><code class="language-bash">Host mlops-zoomcamp
    HostName EXTERNAL_IP
    User USER
    IdentityFile KEY_FILENAME_DIRECTORY
    LocalForward PORT_1 IP:PORT_1
    LocalForward PORT_2 IP:PORT_2
    LocalForward PORT_3 IP:PORT_3
</code></pre>
<p><strong>Example:</strong></p>
<pre><code class="language-bash">Host mlops-zoomcamp
    HostName 18.117.147.165
    User ubuntu
    IdentityFile C:\Users\ferro\.ssh\asus-laptop.pem
    LocalForward 8888 localhost:8888
    LocalForward 5000 127.0.0.1:5000
    LocalForward 4200 0.0.0.0:4200
</code></pre>
</li>
</ol>
<p>The EXTERNAL_IP can change every time you power one the VM. 
Now you can type <code>ssh mlops-zoomcamp</code> in your console and you'll get connected to the VM.</p>
<p><strong>Note0</strong>: In step 4 in <code>config</code> file the last two lines are to forward multiple port through the same host, in this case 
<code>LocalForward 8888 localhost:8888</code> is for jupyter, <code>LocalForward 5000 127.0.0.1:5000</code> is for MLflow and <code>LocalForward 4200 0.0.0.0:4200</code> is for Prefect. You can add more LocalForward if you want.</p>
<p><strong>Note1</strong>: Don't forget to power off the VM after your work you can use <code>sudo poweroff</code>. </p>
<p><strong>Note2</strong>: if you get the next warning:</p>
<pre><code class="language-bash">@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
</code></pre>
<p>Then copy and paste the EXTERNAL_IP of your VM and type:</p>
<pre><code class="language-bash">ssh-keygen -R EXTERNAL_IP
</code></pre>
<p>Example:</p>
<pre><code class="language-bash">ssh-keygen -R &quot;34.125.105.3&quot;
</code></pre>
<p><a href="Module1/2_aws.html#">Back to the top</a></p>
<h3 id="vs-code-setup"><a class="header" href="#vs-code-setup">VS Code Setup <a name="vscode"></a></a></h3>
<p>If you want to use the VM with a local VS code, follow:</p>
<ol>
<li>Install &quot;Remote - SSH&quot; extension in VS Code</li>
<li>Click on &quot;Open a Remote Window&quot; icon on bottom-left corner</li>
<li>From dropdown select &quot;Connect to Host&quot; and then select the host name that you put in the <code>config</code> file, in this case <code>mlops-zoomcamp</code>. That opens a new VSCode window.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="install-dependencies"><a class="header" href="#install-dependencies">Install Dependencies</a></h1>
<p>Now we need to install the next dependencies(you can update the links for Anaconda and Docker Compose):</p>
<ul>
<li><strong>Install Anaconda</strong>:</li>
</ul>
<pre><code class="language-bash">cd ~
wget https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh
bash Anaconda3-2022.05-Linux-x86_64.sh
</code></pre>
<ul>
<li><strong>Install Docker</strong>:</li>
</ul>
<pre><code class="language-bash">sudo apt update
sudo apt install docker.io
</code></pre>
<ul>
<li><strong>Install Docker Compose</strong></li>
</ul>
<pre><code class="language-bash">mkdir soft
cd soft/
wget https://github.com/docker/compose/releases/download/v2.5.0/docker-compose-linux-x86_64 -O docker-compose
chmod +x docker-compose
</code></pre>
<ul>
<li>
<p><strong>Modified PATH Varibles</strong></p>
<p>Type <code>cd</code> to return to the original directory and type <code>nano .bashrc</code>, copy and paste the next at the end of the <code>.bashrc</code> file</p>
</li>
</ul>
<pre><code class="language-bash">export PATH=&quot;${HOME}/soft:${PATH}&quot;
</code></pre>
<p>Type <code>source .bashrc</code>, now everything that is in <code>/soft</code> directory will be in the PATH then you can execute it everywhere.</p>
<ul>
<li><strong>Add current user to docker group</strong></li>
</ul>
<pre><code class="language-bash">sudo usermod -aG docker $USER
logout
</code></pre>
<p>Then logback to the VM.</p>
<ul>
<li><strong>Verify Installation</strong></li>
</ul>
<pre><code class="language-bash">which python
# /home/ubuntu/anaconda3/bin/python

which docker
# /usr/bin/docker

which docker-compose
# /home/ubuntu/soft/docker-compose

docker run hello-world
</code></pre>
<ul>
<li><strong>Run Jupyter Notebook</strong></li>
</ul>
<pre><code class="language-bash">jupyter notebook
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="course-overview"><a class="header" href="#course-overview">Course Overview</a></h1>
<p><a href="https://www.youtube.com/watch?v=teP9KWkP6SM&amp;list=PL3MmuxUbc_hIUISrluw_A7wDSmfOhErJK&amp;index=6">Video source</a></p>
<p>When data scientists experiment with Jupyter Notebooks for creating models, they often don't follow best practices and are often unstructured due to the nature of experimentation: cells are re-run with slightly different values and previous results may be lost, or the cell execution order may be inconsistent, for example.</p>
<p><em><strong>Module 2</strong></em> covers <em><strong>experiment tracking</strong></em>: by using tools such as <a href="https://mlflow.org/">MLflow</a> we will create <em><strong>experiment trackers</strong></em> (such as the history of cells that we've rerun multiple times) and <em><strong>model registries</strong></em> (for storing the models we've created during the experiments), instead of relying on our memory or janky setups such as external spreadsheets or convoluted naming schemes for our files.</p>
<p><em><strong>Module 3</strong></em> covers <em><strong>orchestration and ML pipelines</strong></em>: by using tools such as <a href="https://www.prefect.io/">Prefect</a> and <a href="https://www.kubeflow.org/">Kubeflow</a> we can break down our notebooks into separate identifyable steps and connect them in order to create a <em><strong>ML pipeline</strong></em> which we can parametrize with the data and models we want and easily execute.</p>
<p><img src="Module1/../Images/Module1/ML-pipeline.PNG" alt="asda" /></p>
<p><em><strong>Module 4</strong></em> covers <em><strong>serving the models</strong></em>: we will learn how to deploy models in different ways.</p>
<p><em><strong>Module 5</strong></em> covers <em><strong>model monitoring</strong></em>: we will see how to check whether our model is performing fine or not and how to generate alers to warn us of performance drops and failures, and even automate retraining and redeploying models without human input.</p>
<p><em><strong>Module 6</strong></em> covers <em><strong>best practices</strong></em>, such as how to properly maintain and package code, how to deploy successfully, etc.</p>
<p><em><strong>Module 7</strong></em> covers <em><strong>processes</strong></em>: we will see how to properly communicate between all the stakeholders of a ML project (scientists, engineers, etc) and how to work together.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mlops-maturity-model"><a class="header" href="#mlops-maturity-model">MLOps Maturity Model</a></h1>
<p><a href="https://www.youtube.com/watch?v=XwTH8BDGzYk&amp;list=PL3MmuxUbc_hIUISrluw_A7wDSmfOhErJK&amp;index=7">Video Source</a></p>
<p><a href="https://docs.microsoft.com/en-us/azure/architecture/example-scenario/mlops/mlops-maturity-model">Table Source</a></p>
<p>A framework for classifying different levels of MLOps maturity is listed below:</p>
<table><thead><tr><th align="right">Lvl</th><th></th><th>Overview</th><th>Use Case</th></tr></thead><tbody>
<tr><td align="right">0️⃣</td><td><strong>No MLOps</strong></td><td><ul><li>ML process highly manual</li><li>poor cooperation</li><li>lack of standards, success depends on an individual's expertise</li> </ul></td><td><ul><li>proof of concept (PoC)</li><li>academic project</li></ul></td></tr>
<tr><td align="right">1️⃣</td><td><strong>DevOps but no MLOps</strong></td><td><ul><li>ML training is most often manual </li><li>software engineers might help with the deployment</li><li>automated tests and releases</li> </ul></td><td><ul><li>bringing PoC to production</li></ul></td></tr>
<tr><td align="right">2️⃣</td><td><strong>Automated Training</strong></td><td><ul><li>ML experiment results are centrally tracked </li><li>training code and models are version controlled</li><li>deployment is handled by software engineers</li> </ul></td><td><ul><li>maintaining 2-3+ ML models</li></ul></td></tr>
<tr><td align="right">3️⃣</td><td><strong>Automated Model Deployment</strong></td><td><ul><li>releases are managed by an automated CI/CD pipeline</li><li>close cooperation between data and software engineers</li><li>performance of the deployed model is monitored, A/B tests for model selection are used</li></ul></td><td><ul><li>business-critical ML services</li></ul></td></tr>
<tr><td align="right">4️⃣</td><td><strong>Full MLOps Automated Operations</strong></td><td><ul><li>clearly defined metrics for model monitoring</li><li>automatic retraining triggered when passing a model metric's threshold</li> </ul></td><td><ul><li>use only when a favorable trade-off between implementation cost and increase in efficiency is likely</li><li>retraining is needed often and is repetitive (has potential for automation)</li></ul></td></tr>
</tbody></table>
<p>Be aware that not every project or even every part of a project needs to have the highest maturity level possible because it could exceed the project's resource budget. <strong>Pragmatism is key</strong>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="introduction-to-experiment-tracking"><a class="header" href="#introduction-to-experiment-tracking">Introduction to Experiment tracking</a></h1>
<h2 id="important-concepts"><a class="header" href="#important-concepts">Important concepts</a></h2>
<ul>
<li><strong>ML experiment:</strong> the process of building an ML model; The whole process in which a Data Scientist creates and optimizes a model</li>
<li><strong>Experiment run:</strong> each trial in an ML experiment; Each run is within an ML experiment</li>
<li><strong>Run artifact:</strong> any file associated with an ML run: Examples include the model itself, package versions...etc; Each Artifact is tied to an Experiment</li>
<li><strong>Experiment metadata:</strong> metadata tied to each experiment</li>
</ul>
<h2 id="experiment-tracking"><a class="header" href="#experiment-tracking">Experiment tracking</a></h2>
<p>Experiment tracking is the process of <strong>keeping track of all the relevant information from an ML experiment</strong>(relevant information depends on the experiment), which includes source code, environment, data, model, hyperparameters, metrics, etc.</p>
<p>Experiment tracking helps with:</p>
<ul>
<li><strong>Reproducibility</strong></li>
<li><strong>Organization</strong></li>
<li><strong>Optimization</strong></li>
</ul>
<p><strong>Note:</strong> Tracking experiments in spreadsheets helps but is not enough because <strong>error prone</strong>, <strong>no standard format</strong>, <strong>no visibility and collaboration</strong>.</p>
<h2 id="mlflow"><a class="header" href="#mlflow">MLflow</a></h2>
<p><img src="Module2/../Images/Module2/mlflow_logo.png" alt="conda_logo" /></p>
<p><em>&quot;An Open source platform for the machine learning lifecycle&quot;</em></p>
<p>Where ml lifecycle means the whole process of building and maintaining ml models. </p>
<p>MLflow is a python package that contains four main modules:</p>
<ul>
<li><strong>Tracking</strong></li>
<li><strong>Models</strong></li>
<li><strong>Model registry</strong></li>
<li><strong>Projects (Out of scope of this notes)</strong></li>
</ul>
<p>You can check de official <a href="https://www.mlflow.org/docs/latest/index.html">MLflow Documentation</a></p>
<h3 id="tracking-experiments-with-mlflow"><a class="header" href="#tracking-experiments-with-mlflow">Tracking experiments with MLflow:</a></h3>
<p>The MLflow Tracking module allows you to organize your experiments into runs, and to keep track of:</p>
<ul>
<li><strong>Parameters:</strong> Any parameter that affects the model like <em>hyperparameters</em>,  <em>training dataset path</em>, <em>preprocessing</em>, etc.</li>
<li><strong>Metrics:</strong> Metrics for training and test dataset</li>
<li><strong>Metadata:</strong> For example <em>tags</em></li>
<li><strong>Artifacts:</strong> Any file like visualizations</li>
<li><strong>Models</strong></li>
</ul>
<p>Along with this information, MLflow automatically logs extra information about the run:</p>
<ul>
<li><strong>Source code</strong></li>
<li><strong>Version of the code (git commit)</strong></li>
<li><strong>Start and end time</strong></li>
<li><strong>Author</strong></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started-with-mlflow"><a class="header" href="#getting-started-with-mlflow">Getting Started with MLflow</a></h1>
<p>You can create a <a href="Module2/../Appendixes/B_conda.html">conda environment</a> and install MLlow into it, install with:</p>
<pre><code class="language-bash">pip install mlflow=1.27
</code></pre>
<p>MLflow has different interfaces, each with their pros and cons. We introduce the core functionalities of MLflow through the UI.</p>
<h2 id="mlflow-ui"><a class="header" href="#mlflow-ui">MLflow UI:</a></h2>
<p>To run the MLflow UI locally we use the command:</p>
<pre><code>mlflow ui --backend-store-uri sqlite:///mlflow.db
</code></pre>
<p><strong>Note0:</strong> If you don't specified a backend store you can't use the Model Registry.</p>
<p><strong>Note1:</strong> In the directory you ran the command will be created a <code>mlruns</code> directory and <code>mlflow.db</code> file.</p>
<p><strong>Note2:</strong> The backend storage is essential to access the features of MLflow, in this command we use a SQLite backend with the file <code>mlflow.db</code> in the current running repository. This URI is also given later to the MLflow Python API <code>mlflow.set_tracking_uri</code>.</p>
<p><strong>Note3:</strong> While the <strong>backend store</strong> persists MLflow entities (runs, parameters, metrics, tags, notes, metadata, etc), the <strong>artifact store</strong> persists artifacts (files, models, images, in-memory objects, or model summary, etc).</p>
<p>By accessing the provided local url in the terminal you can access to the UI and you'll see something like this:</p>
<p><img src="Module2/../Images/Module2/mlflow.PNG" alt="conda_logo" /></p>
<p>In addition to the backend URI, we can also add an artifact root directory where we store the artifacts for runs, this is done by adding a <code>--default-artifact-root</code> paramater:</p>
<pre><code>mlflow ui --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tracking-a-single-experiment-run"><a class="header" href="#tracking-a-single-experiment-run">Tracking a Single Experiment run</a></h1>
<p>In order to track experiment runs, we first initialize the mlflow experiment setting the <em>tracking uri</em> and the <em>experiment</em> using the code:</p>
<pre><code class="language-python">import mlflow
mlflow.set_tracking_uri(&quot;sqlite:///mlflow.db&quot;)
mlflow.set_experiment(EXPERIMENT_NAME)
</code></pre>
<p>Example:</p>
<pre><code class="language-python">import mlflow
mlflow.set_tracking_uri(&quot;sqlite:///mlflow.db&quot;)
mlflow.set_experiment(&quot;nyc-taxi-experiment&quot;)
</code></pre>
<p>Where <code>EXPERIMENT_NAME</code> is the name of the experiment you want to use(it will be created if doesn't exist')</p>
<p>We can then track a run, we'll use this simple code snippet as a starting point:</p>
<pre><code class="language-python">alpha = 0.01

lr = Lasso(alpha)
lr.fit(X_train, y_train)

y_pred = lr.predict(X_val)

mean_squared_error(y_val, y_pred, squared = False)
</code></pre>
<p>We initialize the run using <code>with mlflow.start_run():</code> and wrapping the whole run inside it. We track the relevant information using  three mlflow commands:</p>
<ul>
<li><code>set_tag</code> for Metadata tags</li>
<li><code>log_param</code> for logging model parameters</li>
<li><code>log_metric</code> for logging model metrics</li>
</ul>
<p>In the next example we set:</p>
<ol>
<li><strong>Metadata tags:</strong> the author name</li>
<li><strong>Log parameters:</strong> training and validation data paths and alpha value</li>
<li><strong>Log metrics:</strong> RMSE</li>
</ol>
<p><strong>Note:</strong> The python script or notebooks have to be runned in the same directory as you will run the MLflow UI.</p>
<pre><code class="language-python">with mlflow.start_run():
    mlflow.set_tag(&quot;developer&quot;, &quot;Chris&quot;)
    
    mlflow.log_param(&quot;train-data-path&quot;, &quot;data/green_tripdata_2021-01.parquet&quot;)
    mlflow.log_param(&quot;val-data-path&quot;, &quot;data/green_tripdata_2021-02.parquet&quot;)
    
    alpha = 0.01
    mlflow.log_param(&quot;alpha&quot;, alpha)
    lr = Lasso(alpha)
    lr.fit(X_train, y_train)
    
    y_pred = lr.predict(X_val)
    rmse = mean_squared_error(y_val, y_pred, squared = False)
    mlflow.log_metric(&quot;rmse&quot;, rmse)
</code></pre>
<p>Now if we run this then in the MLflow UI you'll see a new experiment <code>nyc-taxi-experiment</code>and inside it will be a new run with our logged parameters, tag, and metric.</p>
<p><img src="Module2/../Images/Module2/mlflow_experiment.PNG" alt="conda_logo" /></p>
<p>If you click in the time ago fo the run you'll se all the logs.</p>
<p><img src="Module2/../Images/Module2/mlflow_parameters.PNG" alt="conda_logo" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tracking-a-multiple-experiment-runshyperparameter-optimization"><a class="header" href="#tracking-a-multiple-experiment-runshyperparameter-optimization">Tracking a Multiple Experiment runs(Hyperparameter Optimization)</a></h1>
<p>If you want to track Hyperparameter Optimization you can do a for loop with <code>with mlflow.start_run():</code> inside and try a different hyperparameters in each iteration, but this is not a efficient way, then we are going to use <code>hyperopt</code> this python library use bayesian methods to find the best hyperparameters in a efficient way. If you want to know more about hyperopt you can chek de <a href="http://hyperopt.github.io/hyperopt/">Hyperopt Documentation</a>. We import some classes from <code>hyperopt</code>:</p>
<ul>
<li><code>fmin</code>: Try to minimize and output(a metric)</li>
<li><code>tpe</code>: Algorithm to control the logic of the optimization</li>
<li><code>hp</code>: Include different methods to define the search space</li>
<li><code>STATUS_OK</code>: Signal to tell hyperopt the objetive function has run successfully</li>
<li><code>Trials</code>: Keep track information of each run</li>
<li><code>scope</code>: Define a range of type integer</li>
</ul>
<p>And now by wrapping the hyperopt Optimization objective inside a with <code>with mlflow.start_run()</code> block, we can track every optimization run that was ran by hyperopt. We then log the parameters passed by hyperopt as well as the metric as follows:</p>
<pre><code class="language-python">
import xgboost as xgb

from hyperopt import fmin, tpe, hp, STATUS_OK, Trials
from hyperopt.pyll import scope

train = xgb.DMatrix(X_train, label=y_train)
valid = xgb.DMatrix(X_val, label=y_val)

def objective(params):
    with mlflow.start_run():
        mlflow.set_tag(&quot;model&quot;, &quot;xgboost&quot;)
        mlflow.log_params(params)
        booster = xgb.train(
            params=params,
            dtrain=train,
            num_boost_round=1000,
            evals=[(valid, 'validation')],
            early_stopping_rounds=50
        )
        y_pred = booster.predict(valid)
        rmse = mean_squared_error(y_val, y_pred, squared=False)
        mlflow.log_metric(&quot;rmse&quot;, rmse)

    return {'loss': rmse, 'status': STATUS_OK}

search_space = {
    'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),
    'learning_rate': hp.loguniform('learning_rate', -3, 0),
    'reg_alpha': hp.loguniform('reg_alpha', -5, -1),
    'reg_lambda': hp.loguniform('reg_lambda', -6, -1),
    'min_child_weight': hp.loguniform('min_child_weight', -1, 3),
    'objective': 'reg:linear',
    'seed': 42
}

best_result = fmin(
    fn=objective,
    space=search_space,
    algo=tpe.suggest,
    max_evals=50,
    trials=Trials()
)

</code></pre>
<p>In this block, we defined the search space and the objective than ran the optimizer. We wrap the training and validation block inside <code>with mlflow.start_run()</code> and log the used parameters using <code>log_params</code> and validation RMSE using <code>log_metric</code>.</p>
<p>In the UI will be the runs generated by hyperopt, since we use the tag <code>mlflow.set_tag(&quot;model&quot;, &quot;xgboost&quot;)</code> you can filter the runs typingin the search section(in the UI):</p>
<pre><code class="language-bash">tags.model = &quot;xgboost&quot;
</code></pre>
<p>In the UI you can compare all this runs following <strong>Filter xboost models -&gt; Select all of them -&gt; CLick in Compare</strong> and you will can visualize a Parallel Coordinates Plot, Scatter Plot and Contour Plot.</p>
<p><img src="Module2/../Images/Module2/mlflow_viz.PNG" alt="conda_logo" /></p>
<p>If you want to know how to read this kind of visualizations you can check this <a href="https://youtu.be/iaJz-T7VWec?list=PL3MmuxUbc_hIUISrluw_A7wDSmfOhErJK&amp;t=684">video</a></p>
<h2 id="selecting-the-best-model"><a class="header" href="#selecting-the-best-model">Selecting the Best Model</a></h2>
<p>When you select the best model is importan to check not just the performance, is important to check:</p>
<ul>
<li><strong>Performance</strong></li>
<li><strong>Training time</strong></li>
<li><strong>Model Size(Complexity)</strong></li>
</ul>
<p>Could be that a model has the best performance but the training time and the model size are very bigger than the other model, in that case could be better to choose a model with lower performance but a lower training time and model size.</p>
<p>Onece you have select the best model you have to run again the model with the best parameters, you can use autologging to avoid write the mlflow logs.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="autologging"><a class="header" href="#autologging">Autologging:</a></h1>
<p>Instead of logging the parameters by &quot;Hand&quot; by specifiying the logged parameters and passing them. We may use the Autologging feature in MLflow. There are two ways to use Autologging; First by enabling it globally in the code/Notebook using </p>
<pre><code class="language-python">mlflow.autolog()
</code></pre>
<p>or by enabling the framework-specific autologger, example with XGBoost:</p>
<pre><code class="language-python">mlflow.xgboost.autolog()
</code></pre>
<p>Both must be done before running the experiments like. You can check the frameworks supported by autologger in <a href="https://www.mlflow.org/docs/latest/tracking.html#automatic-logging">Automatic Logging</a></p>
<p>An example of how to use autologging(We don't need <code>with mlflow.start_run():</code>) in MLflow is the next example belong to the best model selected:</p>
<pre><code class="language-python">best_params = {
    &quot;learning_rate&quot;: 0.19000742747973715,
    &quot;max_depth&quot;: 44,
    &quot;min_child_weight&quot;: 3.852547711639823,
    &quot;objective&quot;: &quot;reg:linear&quot;,
    &quot;reg_alpha&quot;: 0.006755095335696288,
    &quot;reg_lambda&quot;: 0.1890415913639682,
    &quot;seed&quot;:	42,
}

mlflow.xgboost.autolog()

booster = xgb.train(
            params=best_params,
            dtrain=train,
            num_boost_round=1000,
            evals=[(valid, 'validation')],
            early_stopping_rounds=50
        )

</code></pre>
<p>The autologger then not only stores the model parameters for ease of use, it also stores other files inside the <code>model</code> (can be specified) folder inside our experiment artifact folder, these files include:</p>
<ul>
<li><code>conda.yaml</code> and <code>requirements.txt</code>: Files which define the current envrionment for use with either <code>conda</code> or <code>pip</code> respectively</li>
<li><code>MLmodel</code> an internal MLflow file for organization</li>
<li>Other framework-specific files such as the model itself</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="logging-models"><a class="header" href="#logging-models">Logging Models</a></h1>
<h2 id="saving-models"><a class="header" href="#saving-models">Saving Models</a></h2>
<p>After we select the best model we can log whole models for storage (see Model Registry later), to do this we add a line at the end of our <code>with mlflow.start_run()</code> block:</p>
<pre><code class="language-python">mlflow.&lt;framework&gt;.log_model(model, artifact_path=&quot;models_mlflow&quot;)
</code></pre>
<p>Where we replace the <code>&lt;framework&gt;</code> wih our model's framework (<code>sklearn</code>, <code>xgboost</code>, ... etc.).
The <code>artifact_path</code> defines where in the <code>artifact_uri</code> the model is stored.</p>
<p>We now have our model inside our <code>models_mlflow</code> directory in the experiment folder. </p>
<p><strong>Note:</strong> Using Autologging would store more data on parameters as well as the model. i.e: Use <code>log_model </code> is redundant when using the autologger.</p>
<h2 id="saving-artifacts-with-the-model"><a class="header" href="#saving-artifacts-with-the-model">Saving Artifacts with the Model</a></h2>
<p>Sometimes we may want to save some artifacts with the model, for example in our case we may want to save the <code>DictVectorizer</code> object with the model for inference (subsequently testing as well). In that case we save the artifact as:</p>
<pre><code class="language-python">mlflow.log_artifact(&quot;vectorizer.pkl&quot;, artifact_path = &quot;extra_artifacts&quot;)
</code></pre>
<p>Where <code>vectorizer.pkl</code> is the vectorizer stored in a Pickle file and <code>extra_artifacts</code> the folder within the artifacts of the model where the file is stored.</p>
<p>An example of saving artifacts with the model:</p>
<pre><code class="language-python">with mlflow.start_run():

    train = xgb.DMatrix(X_train, label=y_train)
    valid = xgb.DMatrix(X_val, label=y_val)
    
    best_params = {
        &quot;learning_rate&quot;: 0.19000742747973715,
        &quot;max_depth&quot;: 44,
        &quot;min_child_weight&quot;: 3.852547711639823,
        &quot;objective&quot;: &quot;reg:linear&quot;,
        &quot;reg_alpha&quot;: 0.006755095335696288,
        &quot;reg_lambda&quot;: 0.1890415913639682,
        &quot;seed&quot;:	42,
    }

    mlflow.log_params(best_params)

    booster = xgb.train(
                params=best_params,
                dtrain=train,
                num_boost_round=1000,
                evals=[(valid, 'validation')],
                early_stopping_rounds=50
            )

    y_pred = booster.predict(valid)
    rmse = mean_squared_error(y_val, y_pred, squared = False)
    mlflow.log_metric(&quot;rmse&quot;, rmse)

    with open(&quot;vectorizer.pkl&quot;, &quot;wb&quot;) as f_out:
        pickle.dump(dv, f_out)
    
    mlflow.log_artifact(&quot;vectorizer.pkl&quot;, artifact_path = &quot;extra_artifacts&quot;)
    mlflow.xgboost.log_model(booster, artifact_path = &quot;models_mlflow&quot;)
</code></pre>
<h2 id="loading-models"><a class="header" href="#loading-models">Loading Models:</a></h2>
<p>We can use the model to make predictions with multiple ways depending on what we need:
+ We may load the model as a Spark UDF (User Defined Function) for use with Spark Dataframes
+ We may load the model as a MLflow PyFuncModel structure, to then use to predict data in a Pandas DataFrame, NumPy Array or SciPy Sparse Array. The obtained interface is general for all models from all frameworks
+ We may load the model as is, i.e: load the XGBoost model as an XGBoost model and treat it as such</p>
<p>The first two methods are explained briefly in the MLflow artifacts page for each run, for the latter we may use (XGBoost example):</p>
<pre><code class="language-python">logged_model = 'runs:/9245396b47c94513bbf9a119b100aa47/models' # Model UUID from the MLflow Artifact page for the run

xgboost_model = mlflow.xgboost.load_model(logged_model)
</code></pre>
<p>The resultant <code>xgboost_model</code> is an XGBoost <code>Booster</code> object which behaves like any XGBoost model. We can predict as normal and even use XGBoost Booster functions such as <code>get_fscore</code> for feature importance.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="model-registry"><a class="header" href="#model-registry">Model Registry</a></h1>
<p>The Data Scientist decides what models in the tracking server are ready to production and then can register those models into the mlflow registry in that way the Deployment Engineer can take a look in the model registry and see whats models are ready for production. In that way the communication between the person that is in charge of building the model and the person is in charge of deploying the model is improved.</p>
<p><img src="Module2/../Images/Module2/mlflow_registry.PNG" alt="conda_logo" /></p>
<p>Model Registry only lists models that are production-ready(don't perform the deployment <em>perse</em>) and the stages are just labels assigned to the model. To deploy the model, we will require to implement CI/CD code to communicate with Model Registry.</p>
<p><strong>Note:</strong> To use the Model Registry you need to specify a backend store.</p>
<h2 id="model-registry-using-ui"><a class="header" href="#model-registry-using-ui">Model Registry using UI</a></h2>
<p><strong>Storing Models in the Registry</strong></p>
<p>In order to register models using the UI, we select the run whose model we want to register and then select &quot;Register Model&quot;. There we may either create a new model registry or register the model into an existing registry. We can view the registry and the models there in by selecting the &quot;Models&quot; tab in the top and selecting the registry we want.</p>
<p><img src="Module2/../Images/Module2/mlflow_modelstab.png" alt="conda_logo" /></p>
<p><strong>Promoting and Demoting Models in the registry</strong></p>
<p>Models in the registry are labeled either as Staging, Production or Archive. Promoting and demoting a model can be done by selecting the model in the registry and selecting the stage of the model in the drop down &quot;Stage&quot; Menu at the top.</p>
<p><img src="Module2/../Images/Module2/mlflow_modelstage.png" alt="conda_logo" /></p>
<h2 id="model-registry-using-tracking-client"><a class="header" href="#model-registry-using-tracking-client">Model Registry using Tracking Client</a></h2>
<p><strong>Create MLflow Client Instance</strong></p>
<p>In order to automate the process of registering/promoting/demoting models, we use the Tracking Client API initialized as described above:</p>
<pre><code class="language-python">from mlflow.tracking import MlflowClient

MLFLOW_TRACKING_URI = &quot;sqlite:///mlflow.db&quot;

client = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)
</code></pre>
<p><strong>Get List of Experiments</strong></p>
<p>You can get a list of the experiments with its <code>artifact_location</code>, <code>experiment_id</code>, <code>tags</code>, etc. Typing:</p>
<pre><code class="language-python">client.list_experiments()
</code></pre>
<p><strong>Note:</strong> Every time you start using mlflow there will be a default experiment <code>name = Default</code>.</p>
<p><strong>Create and Experimment</strong></p>
<pre><code class="language-python">client.create_experiment(name = EXPERIMENT_NAME)
</code></pre>
<p><strong>Searching Runs</strong></p>
<p>We can search for runs by ascending order of metric score using the API following the next example:</p>
<pre><code class="language-python">from mlflow.entities import ViewType

runs = client.search_runs(
    experiment_ids = '1',    # Experiment ID we want
    filter_string = &quot;metrics.rmse &lt; 7&quot;,
    run_view_type = ViewType.ACTIVE_ONLY,
    max_results = 5,
    order_by = [&quot;metrics.rmse ASC&quot;]
)
</code></pre>
<ul>
<li><code>search_runs</code> is a simplified version of SQL <code>WHERE</code> clause.</li>
<li>We first need to specify which experiment id that we are referring to (<code>nyc_taxi_experiment</code>'s id is 1).</li>
<li>The <code>filter_string</code> allows us to filter the runs you can set empty <code>&quot;&quot;</code>.</li>
<li>The <code>run_view_type</code> value <code>ViewType.ACTIVE_ONLY</code> shows only the active runs (and not deleted runs)</li>
<li><code>max_results</code> showing only 5 results</li>
<li><code>order_by</code> - ordering the results by <code>metrics.rmse</code> and <code>ASC</code> ascending (like SQL)</li>
</ul>
<p>Now we can then get information about the selected runs from the resulting <code>runs</code> enumerator:</p>
<pre><code class="language-python">for run in runs:
    print(f&quot;run id: {run.info.run_id}, rmse: {run.data.metrics['rmse']:.4f}&quot;)
</code></pre>
<p><strong>Add a run model to a registry</strong></p>
<pre><code class="language-python">mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

run_id = &quot;9245396b47c94513bbf9a119b100aa47&quot;
model_uri = f&quot;runs:/{run_id}/models&quot;
mlflow.register_model(model_uri=model_uri, name=&quot;nyc-taxi-regressor&quot;)
</code></pre>
<p><strong>Get the models in a model registry</strong></p>
<pre><code class="language-python">model_name = &quot;nyc-taxi-regressor&quot;
latest_versions = client.get_latest_versions(name=model_name)

for version in latest_versions:
    print(f&quot;version: {version.version}, stage: {version.current_stage}&quot;)
</code></pre>
<p><strong>Promote a model to staging</strong></p>
<pre><code class="language-python">model_version = 4
new_stage = &quot;Staging&quot;
client.transition_model_version_stage(
    name=model_name,
    version=model_version,
    stage=new_stage,
    archive_existing_versions=False
)
</code></pre>
<p><strong>Update the description of a model</strong></p>
<pre><code class="language-python">from datetime import datetime

date = datetime.today().date()
client.update_model_version(
    name=model_name,
    version=model_version,
    description=f&quot;The model version {model_version} was transitioned to {new_stage} on {date}&quot;
)
</code></pre>
<p>These can then be used to automate the promotion of packages into production or the archival of older models.</p>
<p><strong>Download Artifacts</strong></p>
<p>You can download the artifacts in <code>path</code> belong to the <code>run_id</code> and will be downloaded in <code>dst_path</code></p>
<pre><code class="language-python">run_id = &quot;9245396b47c94513bbf9a119b100aa47&quot;
client.download_artifacts(run_id = run_id, path = &quot;extra_artifacts&quot;, dst_path = &quot;.&quot;)
</code></pre>
<p>And now you can load the files that were located in the artifacts:</p>
<pre><code class="language-python">import pickled

with open(&quot;vectorizer.pkl&quot;, &quot;rb&quot;) as f_in:
    dv = pickle.load(f_in)
</code></pre>
<p>If you want to know more about MLflow in practice you can check <a href="https://www.youtube.com/watch?v=1ykg4YmbFVA&amp;list=PL3MmuxUbc_hIUISrluw_A7wDSmfOhErJK">MLflow in practice</a>, if you do that for the scenario 3 you'll need <a href="Module2/../Appendixes/C_mlflow_aws.html">MLflow on AWS notes</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mlflow-benefits-limitations-and-alternatives"><a class="header" href="#mlflow-benefits-limitations-and-alternatives">MLflow: Benefits, Limitations and Alternatives</a></h1>
<h2 id="tracking-benefits"><a class="header" href="#tracking-benefits">Tracking benefits:</a></h2>
<ul>
<li>The tracking server can be easily deployed to the cloud</li>
<li>Share experiments with other data scientists</li>
<li>Collaborate with others to build and deploy models</li>
<li>Give more visibility of the data science efforts</li>
</ul>
<h2 id="issues-with-running-a-remote-shared-mlflow-server"><a class="header" href="#issues-with-running-a-remote-shared-mlflow-server">Issues with running a remote (shared) MLflow server</a></h2>
<ul>
<li>
<p><strong>Security:</strong></p>
<ul>
<li>Restrict access to the server (e.g. access through VPN) to avoid people outside your organization may get acces to the experiments, runs, models, etc.</li>
</ul>
</li>
<li>
<p><strong>Scalability:</strong></p>
<ul>
<li>
<p>Check <a href="https://github.com/aws-samples/amazon-sagemaker-mlflow-fargate">Deploy MLflow on AWS Fargate</a> scale nicely when the number of users that access(to the mlflow server) increases.</p>
</li>
<li>
<p>Check <a href="https://www.databricks.com/session_eu20/mlflow-at-company-scale">MLflow at Company Scale by Jean-Denis Lesage</a> to scale mlflow to be able to support thousands of experiments runs and also models.</p>
</li>
</ul>
</li>
<li>
<p><strong>Isolation</strong></p>
<ul>
<li>Define standard for naming experiments, models and a set of default tags(developer or team name as a tag)</li>
<li>Restrict access to artifacts (e.g. use s3 buckets living in different AWS accounts)</li>
</ul>
</li>
</ul>
<h2 id="mlflow-limitations-and-when-not-to-use-it"><a class="header" href="#mlflow-limitations-and-when-not-to-use-it">MLflow limitations (and when not to use it)</a></h2>
<ul>
<li><strong>Authentication &amp; Users:</strong> The open source version of MLflow doesn’t provide
any sort of authentication.</li>
<li><strong>Data versioning:</strong> to ensure full reproducibility we need to version the data
used to train the model. MLflow doesn’t provide a built-in solution for that but
there are a few ways to deal with this limitation</li>
<li><strong>Model/Data Monitoring &amp; Alerting:</strong> this is outside of the scope of MLflow
and currently there are more suitable tools for doing this</li>
</ul>
<h2 id="mlflow-alternatives"><a class="header" href="#mlflow-alternatives">MLflow alternatives</a></h2>
<p>There are some paid alternatives to MLflow:</p>
<ul>
<li><a href="https://neptune.ai/">Neptune</a></li>
<li><a href="https://www.comet.com/site/">Comet</a></li>
<li><a href="https://wandb.ai/site">Weights &amp; Biases</a></li>
<li>You can go to <a href="https://neptune.ai/blog/best-ml-experiment-tracking-tools">many more</a> to see a detailed comparation</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="vm-instance-in-gcp"><a class="header" href="#vm-instance-in-gcp">VM Instance in GCP</a></h1>
<p>Before to create an instance in GCP we need to generate a SSH key(if you want to know what SSH is you can check this <a href="https://www.youtube.com/watch?v=RMS5zBYQIqA">video</a>). In your local console(in my case in Git Bash in windows) follow:</p>
<p>You can watch this <a href="https://www.youtube.com/watch?v=ae-CV2KfoN0">video</a> or follow the next instructions.</p>
<ol>
<li>
<p><strong>Create SSH keys:</strong></p>
<p>Create(if you don't have) and go to <code>~/.ssh</code> directory and type:</p>
</li>
</ol>
<pre><code class="language-bash">ssh-keygen -t rsa -f KEY_FILENAME -C USER -b 2048
</code></pre>
<pre><code>Example:
</code></pre>
<pre><code class="language-bash">ssh-keygen -t rsa -f gcp_ssh -C w10 -b 2048
</code></pre>
<p><a href="https://cloud.google.com/compute/docs/connect/create-ssh-keys">Source</a></p>
<ol start="2">
<li>
<p><strong>Put this SSH key in GCP:</strong></p>
<ol>
<li>
<p>Copy public key:</p>
<pre><code class="language-bash">cat KEY_FILENAME.pub
</code></pre>
<p>Example:</p>
<pre><code class="language-bash">cat gcp_ssh.pub
</code></pre>
</li>
<li>
<p>In Cloud console go to <strong>Metadata</strong> -&gt; <strong>EDIT</strong> -&gt; <strong>SSH keys</strong> -&gt; <strong>Add item</strong> -&gt; <strong>Paste public key</strong> -&gt; <strong>Save</strong></p>
</li>
</ol>
</li>
</ol>
<p><em><a href="https://cloud.google.com/compute/docs/connect/add-ssh-keys">Source</a></em></p>
<ol start="3">
<li>
<p><strong>Create VM Instance:</strong></p>
<p>In Cloud console go to <strong>Compute Engine</strong> -&gt; <strong>VM Instances</strong> -&gt; <strong>Create Instance</strong> config the VM as:</p>
<ul>
<li>name: <code>mlops-zoomcamp-vm</code></li>
<li>region: <code>us-west4 (Las Vegas)</code>, zone: <code>us-west4-b</code></li>
<li>serie: <code>E2</code>, type: <code>e2-standard-4</code></li>
<li>boot disk image: <code>Ubuntu 22.04 LTS</code> boot disk type: <code>balanced persistent disk</code> size(gb): <code>30</code></li>
</ul>
</li>
<li>
<p><strong>Connect to VM Instance:</strong> </p>
<p>Go to <code>~./.ssh</code> directory and locate the <code>config</code> type <code>nano ~/.ssh/config</code>copy and paste:</p>
</li>
</ol>
<pre><code class="language-bash">Host mlops-zoomcamp-vm
    HostName EXTERNAL_IP
    User USER
    IdentityFile KEY_FILENAME_DIRECTORY
    LocalForward PORT_1 IP:PORT_1
    LocalForward PORT_2 IP:PORT_2
    LocalForward PORT_3 IP:PORT_3
</code></pre>
<pre><code>Example:
</code></pre>
<pre><code class="language-bash">Host mlops-zoomcamp-vm
    HostName 34.125.197.156
    User w10
    IdentityFile C:\Users\w10\.ssh\gcp_ssh
    LocalForward 8888 localhost:8888
    LocalForward 5000 127.0.0.1:5000
    LocalForward 4200 0.0.0.0:4200
</code></pre>
<pre><code>The EXTERNAL_IP can change every time you power one the VM. 
</code></pre>
<p>Now you can type <code>ssh mlops-zoomcamp-vm</code> in your console and you'll get connected to the VM.</p>
<p><strong>Note0</strong>: In step 4 in <code>config</code> file the last two lines are to forward multiple port through the same host, in this case 
<code>LocalForward 8888 localhost:8888</code> is for jupyter, <code>LocalForward 5000 127.0.0.1:5000</code> is for MLflow and <code>LocalForward 4200 0.0.0.0:4200</code> is for Prefect. You can add more LocalForward if you want.</p>
<p><strong>Note1</strong>: Don't forget to power off the VM after your work you can use <code>sudo poweroff</code>. </p>
<p><strong>Note2</strong>: if you get the next warning:</p>
<pre><code class="language-bash">@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
</code></pre>
<p>Then copy and paste the EXTERNAL_IP of your VM and type:</p>
<pre><code class="language-bash">ssh-keygen -R &quot;34.125.105.3&quot;
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="conda-environments"><a class="header" href="#conda-environments">Conda Environments</a></h1>
<p><img src="Appendixes/../Images/Appendixes/conda_logo.svg" alt="conda_logo" /></p>
<p>Conda is an open source package management system and environment management system that runs on Windows, macOS, Linux and z/OS. Conda quickly installs, runs and updates packages and their dependencies. Conda easily creates, saves, loads and switches between environments on your local computer. It was created for Python programs, but it can package and distribute software for any language.</p>
<p><strong>Note:</strong> To manage Python resources on Mac M1 is recomendable to use <strong>conda.</strong> Using just pip is problematic for some python libraries if you use M1.</p>
<h2 id="installing-conda"><a class="header" href="#installing-conda">Installing Conda</a></h2>
<p>To install conda select the steps for your OS:</p>
<ul>
<li><a href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/windows.html">Windows</a>: Follow just the first 5 steps</li>
<li><a href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/macos.html">macOS</a>: Follow just the first 6 steps</li>
<li><a href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html">Linux</a>: Follow just the first 6 steps</li>
</ul>
<p><strong>Note:</strong> You have to choose between install Miniconda or Anaconda. Miniconda is a free minimal installer for conda. It is a small, bootstrap version of Anaconda that includes only conda, Python, the packages they depend on, and a small number of other useful packages, including pip, zlib and a few others. <strong>I prefer Miniconda than Anaconda</strong>.</p>
<h2 id="installing-miniconda-from-terminal-in-linux"><a class="header" href="#installing-miniconda-from-terminal-in-linux">Installing Miniconda from Terminal in Linux</a></h2>
<p>Installing miniconda on Linux can be a bit tricky the first time you do it completely from the terminal. The following snippet will create a directory to install miniconda into, download the latest python 3 based install script for Linux 64 bit, run the install script, delete the install script, then add a conda initialize to your bash or zsh shell. After doing this you can restart your shell and conda will be ready to go.</p>
<pre><code class="language-bash">mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm -rf ~/miniconda3/miniconda.sh
~/miniconda3/bin/conda init bash
~/miniconda3/bin/conda init zsh

</code></pre>
<p>If you want to know how to do the same in macOS go to <a href="https://engineeringfordatascience.com/posts/install_miniconda_from_the_command_line/">install_miniconda_from_the_command_line</a></p>
<h2 id="create-an-environment"><a class="header" href="#create-an-environment">Create an Environment</a></h2>
<p>It's recommended to not install packages in <code>base</code>environment of <code>conda</code>, to create an independent environment type:</p>
<pre><code class="language-bash">conda create --name ENVIRONMENT_NAME python=PYTHON_VERSION
</code></pre>
<p>Note: You can choose any python version you want, it will be installed if you don't have.</p>
<p>Example:</p>
<pre><code class="language-bash">conda create --name exp-tracking-env python=3.9
</code></pre>
<p>To activate the environment type:</p>
<pre><code class="language-bash">conda activate exp-tracking-env
</code></pre>
<p>To deactivate the environment</p>
<pre><code class="language-bash">conda deactivate
</code></pre>
<h2 id="jupyter-lab-setup"><a class="header" href="#jupyter-lab-setup">Jupyter Lab Setup</a></h2>
<ol>
<li>
<p>Install Jupyter Lab:</p>
<pre><code class="language-bash">conda install -c conda-forge jupyterlab
</code></pre>
</li>
<li>
<p>Add a conda environment:</p>
<p>First activate the environment you want to add, second type:</p>
<pre><code class="language-bash">conda install ipykernel
</code></pre>
<pre><code class="language-bash">ipython kernel install --user --name=ENVIRONMENT_NAME
</code></pre>
</li>
</ol>
<h2 id="snippets"><a class="header" href="#snippets">Snippets</a></h2>
<p><strong>Delete an environment</strong></p>
<pre><code class="language-bash">conda remove --name ENVIRONMENT_NAME --all
</code></pre>
<p><strong>Show all environments</strong></p>
<pre><code class="language-bash">conda info --envs
</code></pre>
<p><strong>Install libraries from a requirements.txt</strong></p>
<pre><code class="language-bash">conda install --file requirements.txt
</code></pre>
<p><strong>Create <code>environment.yml</code> file via conda</strong></p>
<p>Inside your conda environment type:</p>
<pre><code class="language-bash"> conda env export &gt; environment.yml
</code></pre>
<p><strong>Create an environment from the <code>environment.yml</code> file</strong></p>
<pre><code class="language-bash">conda env create -f environment.yml

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mlflow-on-aws"><a class="header" href="#mlflow-on-aws">MLflow on AWS</a></h1>
<p>This tutorials explains how to configure a MLflow remote tracking server on AWS. We will use an RDS database as the backend store and an S3 bucket as the artifact store.</p>
<p><strong>Note:</strong> While the <strong>backend store</strong> persists MLflow entities (runs, parameters, metrics, tags, notes, metadata, etc), the <strong>artifact store</strong> persists artifacts (files, models, images, in-memory objects, or model summary, etc).</p>
<ol>
<li>
<p>First, you need to <a href="https://aws.amazon.com/free">create an AWS account</a>. If you open a new account, AWS allows you to use some of their products for free but take into account that <strong>you may be charged for using the AWS services</strong>. More information <a href="https://youtu.be/rkKvzCskpLE">here</a> and <a href="https://aws.amazon.com/premiumsupport/knowledge-center/free-tier-charges/">here</a>.</p>
</li>
<li>
<p>Launch a new EC2 instance</p>
<p>Go to EC2 service and click in launch instance(orange button), now config the VM as:</p>
<ul>
<li>Name: <code>mlflow_tracking-server</code></li>
<li>Amazon Machine Image: <code>Amazon Linux 2 AMI (HVM) - Kernel 5.10, SSD Volume Type</code></li>
<li>Architecture: <code>64-bit (x86)</code></li>
<li>Instance type: <code>t2.micro</code></li>
<li>Create and select a key pair:
<ul>
<li>Key pair name: <code>mlflow-key-pair</code></li>
<li>Key pair type: <code>RSA</code></li>
<li>Private key file format: <code>.pem</code></li>
</ul>
</li>
<li>Configure Storage: <code>1x 8 Gib gp2 Root Volume</code></li>
</ul>
<p>Finally, you have to edit the security group so the EC2 instance accepts SSH (port 22) and HTTP connections (port 5000) as follow:</p>
<ol>
<li>Go to EC2 instances </li>
<li>Click in Instance ID </li>
<li>Go to &quot;Security&quot; tab </li>
<li>Click in &quot;Security groups&quot;</li>
<li>Click in &quot;Edit inbound rules&quot; </li>
<li>Add a new rule as the image</li>
</ol>
<p><img src="Appendixes/../Images/Module2/mlflow_ec2.png" alt="conda_logo" /></p>
<p>Now you can connect to the VM using the steps in module 1</p>
<p><strong>Note:</strong> You can use the EC2 instance created in the module 1 but in that case you have to apply the last step(Editting the security group). </p>
</li>
<li>
<p>Create an S3 bucket to be used as the <strong>artifact store</strong></p>
<ol>
<li>Go to S3 and click on &quot;Create bucket&quot;</li>
<li>Fill Bucket name, for example with <code>mlflow-artifacts-remote</code></li>
<li>Click on &quot;Create bucket&quot;</li>
</ol>
<p><strong>Note:</strong> S3 bucket names must be unique across all AWS account in all the AWS Regions within a partition, that means that once a bucket is created, the name of that bucket cannot be used by another AWS account within the same region. If you get an error saying that the bucket name was already taken you can fix it easily by just changing the name to something like <code>mlflow-artifacts-remote-2</code> or another name.</p>
</li>
<li>
<p>Create a new PostgreSQL Database(To be used as the <strong>backend store</strong>)</p>
<p>Go to the RDS Console and click on &quot;Create database&quot; and choose the next configuration:</p>
<ul>
<li>Engine Options
<ul>
<li>Engine type: <code>PostgreSQL</code> </li>
<li>Version: <code>PostgreSQL 13.4-R1</code></li>
</ul>
</li>
<li>Templates: <code>Free tier</code></li>
<li>Settings
<ul>
<li>DB instance identifier: <code>mlflow-backend-db</code></li>
<li>Master username: <code>mlflow</code></li>
<li>Auto generate a password: Tick the option</li>
</ul>
</li>
<li>Additional configuration
<ul>
<li>Initial database name: <code>mlflow_db</code></li>
</ul>
</li>
</ul>
<p>You can use the default values for all the other configurations. Now click in &quot;Create database&quot;. Then you'll can view the credentials, its important that you save the credentials beacuse the password will be shown only once!</p>
<p><img src="Appendixes/../Images/Module2/mlflow_credentials.png" alt="conda_logo" /></p>
<p>Take note of the following information:</p>
<ul>
<li><strong>Master username</strong></li>
<li><strong>Master password</strong></li>
<li><strong>Initial database name</strong></li>
<li><strong>Endpoint</strong></li>
</ul>
<p>To get the endpoint <strong>Go to RDS -&gt; Go to DB intances -&gt; Click in the DB identifier -&gt; The endpoint is in the &quot;Connectivity &amp; security&quot; tab</strong>, the endpoint take some time to appear, be patient.</p>
<p>Once the DB instance is created, go to the RDS console, select the new db and under &quot;Connectivity &amp; security&quot; select the VPC security group. Modify the security group by adding a new inbound rule that allows postgreSQL connections on the port 5432 from the security group of the EC2 instance. </p>
<ol>
<li>In RDS go to DB instances</li>
<li>Click in the DB identifier</li>
<li>Go to &quot;Connectivity &amp; security&quot; tab and click in the &quot;VPC security groups&quot;</li>
<li>Click in &quot;Edit inbound rules&quot;</li>
<li>Add a rule as the image</li>
</ol>
<p><img src="Appendixes/../Images/Module2/mlflow_postgresql.png" alt="conda_logo" /></p>
<p>The security group in the right of the <code>Custom</code> should be the same as the security group of the EC2 instance, to know the security group of the EC2 instance:</p>
<ol>
<li>Go to EC2 and click in &quot;Instance ID&quot;</li>
<li>Go to the Security tab and in Security groups is what you want.</li>
</ol>
<p>This way, the server will be able to connect to the postgres database.</p>
<p><strong>Note0:</strong> Thick the option &quot;Auto generate a password&quot; will be Amazon RDS generate a password automatically.</p>
<p><strong>Note1:</strong> Setting a initial database name will be that RDS automatically creates an initial database for you.</p>
<p><strong>Note2:</strong> If you use this for a production project you can consider use a different template like a <code>Production</code> template.</p>
</li>
<li>
<p>Connect to the EC2 Instance and launch the Tracking Server.</p>
<p><strong>EC2 Connection</strong></p>
<p>Start the EC2 instance created in step 2, select it and click in connect and use the easy way click in &quot;EC2 instance connect&quot;.Run the following commands to install the dependencies, configure the environment and launch the server:</p>
<ul>
<li><code>sudo yum update</code></li>
<li><code>pip3 install mlflow boto3 psycopg2-binary</code></li>
<li><code>aws configure</code>   # You'll need to input your AWS credentials here
You will need to input:
<ul>
<li><code>AWS Access Key ID</code></li>
<li><code>AWS Secret Access Key</code></li>
<li><code>Default region name</code>: You can left unchange</li>
<li><code>Default output format</code>: You can left unchange</li>
</ul>
</li>
</ul>
<p>To get your AWS credentials, in the navigation bar on the upper right, choose your account name or number and then choose &quot;Security Credentials&quot;, then expand the &quot;Access keys (access key ID and secret access key)&quot; section, now click in &quot;Create New Access Key&quot; button an the credentials will appear, this is your only opportunity to save your secret access key. After you've saved your secret access key in a secure location, chose Close.</p>
<p>Before launching the server, check that the instance can access the s3 bucket created in the step number 3. To do that, just run this command from the EC2 instance: <code>aws s3 ls</code>. You should see the bucket listed in the result.</p>
<p><strong>Note:</strong> You can't have more than two credentials, if you want a new want you have to delete one.</p>
<p><strong>Launching the server</strong></p>
<pre><code class="language-bash">mlflow server -h 0.0.0.0 -p 5000 --backend-store-uri postgresql://DB_USER:DB_PASSWORD@DB_ENDPOINT:5432/DB_NAME --default-artifact-root s3://S3_BUCKET_NAME
</code></pre>
<ul>
<li><code>DB_USER</code>: The <strong>Master username</strong> that you took note</li>
<li><code>DB_PASSWORD</code>: The <strong>Password username</strong> that you took note</li>
<li><code>DB_ENDPOINT</code>: The <strong>Endpoint</strong> of the PostgreSQL Database</li>
<li><code>DB_NAME</code>: The name of the db</li>
<li><code>S3_BUCKET_NAME</code>: The bucket name in step 3</li>
</ul>
<p>Example:</p>
<ul>
<li><code>DB_USER</code>: mlflow</li>
<li><code>DB_PASSWORD</code>: PLMYm3uAUPumN7LXs0zx</li>
<li><code>DB_ENDPOINT</code>: mlflow-backend-db.cxtpca0hxwbo.us-east-2.rds.amazonaws.com</li>
<li><code>DB_NAME</code>: mlflow_db</li>
<li><code>S3_BUCKET_NAME</code>: mlflow-artifacts-remote-31</li>
</ul>
</li>
<li>
<p>Access the remote tracking server from your local machine.</p>
<p><strong>Access using the UI</strong></p>
<p>Open a new tab on your web browser and go to this address: <code>http://&lt;EC2_PUBLIC_DNS&gt;:5000</code> (you can find the instance's public DNS by checking the details of your instance in the EC2 Console).</p>
<p>For example:</p>
<pre><code>http://ec2-18-191-194-132.us-east-2.compute.amazonaws.com:5000
</code></pre>
<p><strong>Access using Python code</strong></p>
<p>Now you have configure the AWS credentials in the EC2 instance, if you want to use your localhost as the client you need to configure the AWS credentials as well.</p>
<p>Set credentials in the AWS credentials profile file on your local system, located at(if the directory doesn't exist you can create it):</p>
<ul>
<li>
<p><code>~/.aws/credentials</code> on Linux, macOS, or Unix</p>
</li>
<li>
<p><code>C:\Users\USERNAME\.aws\credentials</code> on Windows</p>
</li>
</ul>
<p>This file should contain lines in the following format:</p>
<pre><code class="language-conf">[default]
aws_access_key_id = your_access_key_id
aws_secret_access_key = your_secret_access_key
</code></pre>
<p>Substitute your own AWS credentials values for the values <em>your_access_key_id</em> and <em>your_secret_access_key</em>. Now this profile named as <code>default</code> can be use to set your credentials. </p>
<p><strong>Note:</strong> You can change the name of the profile is not mandatory to use <code>default</code> name.</p>
<p>Now you can acces from python code in your local host, you should be able to run:</p>
<pre><code class="language-python">import mlflow
import os

os.environ[&quot;AWS_PROFILE&quot;] = &quot;default&quot; # Fill in with your AWS profile. More info: https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/setup.html#setup-credentials

TRACKING_SERVER_HOST = &quot;ec2-3-135-9-149.us-east-2.compute.amazonaws.com&quot; # fill in with the public DNS of the EC2 instance
mlflow.set_tracking_uri(f&quot;http://{TRACKING_SERVER_HOST}:5000&quot;)
</code></pre>
<p>Traking a model:</p>
<pre><code class="language-python">from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris
from sklearn.metrics import accuracy_score

mlflow.set_experiment(&quot;my-experiment-1&quot;)

with mlflow.start_run():

    X, y = load_iris(return_X_y=True)

    params = {&quot;C&quot;: 0.1, &quot;random_state&quot;: 42}
    mlflow.log_params(params)

    lr = LogisticRegression(**params).fit(X, y)
    y_pred = lr.predict(X)
    mlflow.log_metric(&quot;accuracy&quot;, accuracy_score(y, y_pred))

    mlflow.sklearn.log_model(lr, artifact_path=&quot;models&quot;)
    print(f&quot;default artifacts URI: '{mlflow.get_artifact_uri()}'&quot;)
</code></pre>
<p>Using Model Registry:</p>
<pre><code class="language-python">from mlflow.tracking import MlflowClient
client = MlflowClient(f&quot;http://{TRACKING_SERVER_HOST}:5000&quot;)

run_id = client.list_run_infos(experiment_id='1')[0].run_id
mlflow.register_model(
    model_uri=f&quot;runs:/{run_id}/models&quot;,
    name='iris-classifier'
)
</code></pre>
</li>
</ol>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
    </body>
</html>
